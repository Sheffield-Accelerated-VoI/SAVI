---
title: "Value of Information Report"
author: "`r input$modelName`"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

# 1. Introduction
Welcome to the Sheffield Accelerated Value of Information (SAVI) application report. The results of your Value of Information analyses in SAVI are reported below. The results are presented in a standardised format to help with the interpretation of your results and future reporting.

In section 2 summary results for the probabilistic sensitivity analysis are presented. Summary statistics, and graphical illustrations are provided to verify the results against previous analysis of the data and illustrate the uncertainty in the model. 

In section 3 the results of the Expected Value of Perfect Information (EVPI) analysis of the data are presented.

In section 4 the results of single parameter Partial Expected Value of Perfect Information (EVPPI) are reported. The results of the analysis of multiple parameter EVPPI, selected in the application are provided. 

```{r, echo=FALSE, results='hide'}
lambda <- lambdaOverall
npsa <- nIterate
```

# 2. Exploring Current Uncertainty: Probabilistic Sensitivity Analysis Results
## 2.1 Probabilistic Sensitivity Analysis Summary
```{r, echo=FALSE, results='asis'}
chosenInt2 <- chosenInt1 <- meanIncCost <- lowerIncCost <-probCostSaving<-meanIncBenefit<-probBenefit<-meanICER<-probCEInt2<-probCEInt1<-probMaxInt<-maxInt<-lowerNbcMaxInt<-upperNbcMaxInt<-meanNbcMaxInt<-percentCEInt2<-percentCEInt1<-baseInt<-99
# Generate PSA table
#  colnames(tableCePlane) <- colnames(costs)[2]
#  table(tableCePlane)
#   incCost <- costs[,2] - costs[,1]
#   incBen <- effects[,2] - effects[,1]
#   npsa <-length(costs[,1])
#   tableCePlane <- matrix(NA,ncol=ncol(costs)-1, nrow = 13)
#   tableCePlane[1] <- format(lambdaOverall, digits=4, nsmall = 0)
#   tableCePlane[2] <- colnames(costs.int)[1]
#   tableCePlane[3] <- format(npsa)
#   tableCePlane[4] <- format(mean(incBen), digits=2, nsmall=4)
#   tableCePlane[5] <- format(mean(incCost), digits=2, nsmall=2)
#   tableCePlane[6] <- format(mean(incCost) /  mean(incBen), digits=2, nsmall=2)
#   tableCePlane[7] <- format(quantile((incBen),0.025), digits=4, nsmall=4)
#   tableCePlane[8] <- format(quantile((incBen),0.975), digits=4, nsmall=4)
#   tableCePlane[9] <- format(quantile((incCost),0.025), digits=4,  nsmall=2)
#   tableCePlane[10] <- format(quantile((incCost),0.975), digits=4, nsmall=2)
#   tableCePlane[11] <- format(length(which((incCost)<0)) / npsa, digits=2, nsmall=3)
#   tableCePlane[12] <- format(length(which((incBen)>0)) / npsa, digits=2, nsmall=3)
#   tableCePlane[13] <- format(length(which((incBen) * lambdaOverall - (incCost)>0)) / npsa, digits=2,
#                              nsmall=3)
#   colnames(tableCePlane) <- colnames(costs)[2]

  incCost <- (costs - costs[, 1])[, -1, drop=FALSE]
  incBen <- (effects - effects[, 1])[, -1, drop=FALSE]
  inb <- incBen * lambda - incCost
  npsa <- NROW(costs)

 makeTableCePlane <- function(costs, effects, lambda) {
  incCost <- (costs - costs[, 1])[, -1, drop=FALSE]
  incBen <- (effects - effects[, 1])[, -1, drop=FALSE]
  inb <- incBen * lambda - incCost
  npsa <- NROW(costs)
  tableCePlane <- matrix(NA, ncol=ncol(costs) - 1, nrow = 13) # incremental, no zero column
  tableCePlane[1, ] <- format(lambda, digits=4, nsmall = 0)
  tableCePlane[2, ] <- colnames(costs)[1]
  tableCePlane[3, ] <- format(npsa)
  tableCePlane[4, ] <- format(colMeans(incBen), digits=2, nsmall=4)
  tableCePlane[5, ] <- format(colMeans(incCost), digits=2, nsmall=2)
  tableCePlane[6, ] <- format(colMeans(incCost) /  colMeans(incBen), digits=2, nsmall=2)
  tableCePlane[7, ] <- format(apply(incBen, 2, quantile, 0.025), digits=4, nsmall=4)
  tableCePlane[8, ] <- format(apply(incBen, 2, quantile, 0.975), digits=4, nsmall=4)
  tableCePlane[9, ] <- format(apply(incCost, 2, quantile, 0.025), digits=4,  nsmall=2)
  tableCePlane[10, ] <- format(apply(incCost, 2, quantile, 0.975), digits=4, nsmall=2)
  tableCePlane[11, ] <- format(apply(incCost, 2, function(x) sum(x > 0)) / npsa, digits=2, nsmall=3)
  tableCePlane[12, ] <- format(apply(incBen, 2, function(x) sum(x > 0)) / npsa, digits=2, nsmall=3)
  tableCePlane[13, ] <- format(apply(inb, 2, function(x) sum(x > 0)) / npsa, digits=2, nsmall=3)
  colnames(tableCePlane) <- colnames(costs)[-1]
  rownames(tableCePlane) <- c(paste("Threshold (", input$currency, ")"), 
                            "Comparator", 
                            "Number of PSA runs", 
                            paste("Mean inc. ",input$unitBens), 
                            paste("Mean inc. Cost (",input$currency,")"),
                            paste("ICER Estimate (", input$currency, "per",input$unitBens,")"),
                            paste("2.5th CI for inc. ",input$unitBens), 
                            paste("97.5th CI for inc. ",input$unitBens),
                            paste("2.5th CI for inc. Costs (",input$currency,")"),
                            paste("97.5th CI for inc. Costs (",input$currency,")"),
                            "Probability cost saving", 
                            "Probability more benefit", 
                            "Probability cost-effective")
  tableCePlane
  }

  print(kable(makeTableCePlane(costs, effects, lambda)))


```

## 2.2 Cost-Effectiveness Plane
The cost-effectiveness plane shows the standardised cost-effectiveness plane per person based on `r nIterate` model runs in which uncertain model parameters are varied simultaneously in a probabilistic sensitivity analysis.  The mean incremental cost of `r colnames(costs[2])` versus `r colnames(costs[1])` is `r input$currency` `r format(colMeans(incCost)[1], digits=2, nsmall=2)`.  There is some uncertainty due to model parameters, with the 95% credible interval for the incremental cost ranging from (`r input$currency` `r format(apply(incCost, 2, quantile, 0.025)[1], digits=4,  nsmall=2)` , `r input$currency` `r format(apply(incCost, 2, quantile, 0.975)[1], digits=4,  nsmall=2)`).  The probability that `r colnames(costs[2])` is cost saving compared to `r colnames(costs[1])` is `r format(apply(incCost, 2, function(x) sum(x > 0))[1] / npsa, digits=2, nsmall=3)`. The mean incremental benefit of `r colnames(costs[2])` versus `r input$current` is `r format(colMeans(incBen)[1], digits=2, nsmall=4)` `r input$unitBens` .  This suggests that `r colnames(costs[2])` is more/or less beneficial.  Again, there is some uncertainty due to model parameters, with the 95% credible interval for the incremental benefit ranging from (`r format(apply(incBen, 2, quantile, 0.025)[1], digits=4, nsmall=4)` `r input$unitBens` , `r format(apply(incBen, 2, quantile, 0.975)[1], digits=4, nsmall=4)` `r input$unitBens`).  The probability that `r colnames(costs[2])` is more beneficial compared to `r colnames(costs[1])` is `r format(apply(incBen, 2, function(x) sum(x > 0))[1] / npsa, digits=2, nsmall=3)`. 

The incremental expected cost per unit of benefit is estimated at `r input$currency` `r format(colMeans(incCost)[1] /  colMeans(incBen)[1], digits=2, nsmall=2)` per `r input$unitBens`.  This is above/below the threshold of `r input$currency` `r format(input$lambdaOverall,digits=4)` per `r input$unitBens` that `r colnames(costs[2])` would (not) be considered cost-effective at this threshold.  There is uncertainty with a `r format(apply(inb, 2, function(x) sum(x > 0))[1] / npsa, digits=2, nsmall=3)` probability that `r colnames(costs[2])` is more cost-effective (`r format((apply(inb, 2, function(x) sum(x > 0))[1] / npsa)*100, digits=2, nsmall=0)` % of the probabilistic model run dots are below and to the right of the diagonal thresholdline).  

```{r, echo=FALSE}
  lambda<-input$lambdaOverall  
  inc_costs <- costs[, 2] - costs[, 1]
  inc_effects <- effects[, 2] - effects[, 1]
  
  m.costs <- max(abs(inc_costs))
  m.effects <- max(abs(inc_effects))
  m2.effects <- m.costs / lambda
  m2.costs <- m.effects * lambda
  m3.costs <- max(m.costs, m2.costs)
  m3.effects <- max(m.effects, m2.effects)
  
  plot(inc_effects, inc_costs, pty="s", cex=0.4,
       ylim=c(-m3.costs, m3.costs), xlim=c(-m3.effects, m3.effects), col="lightblue", 
       xlab = "inc. costs", ylab = paste("inc.", input$unitBens), 
       main = paste("Standardised Cost-effectiveness Plane per Person\nlambda =", lambda))
  abline(1, lambda, lty=2)
  abline(h=0)
  abline(v=0)
  points(mean(inc_effects), mean(inc_costs), pch=20, col="blue", cex=1)
```

See section 5.1 in Briggs, Claxton, Sculpher.  Decision Modelling for Health Economic Evaluation (Handbooks for Health Economic Evaluation). OUP Oxford; 1 edition (17 Aug 2006).  ISBN-13: 978-0198526629

## 2.3 The Cost-Effectiveness Acceptability Curve
The Cost-Effectiveness Acceptability Curve (CEAC) shows the probability that all strategies are cost-effective at varying thresholds.  The results show that at a threshold value for cost-effectiveness of `r input$currency` `r format(input$lambdaOverall, digits = 4)` per `r input$unitBens`, the strategy with the highest probability of being most cost-effective is `r colnames(costs[which.max(ceac.obj$p[21])])`, with a probability of `r max(ceac.obj$p[21])`.

```{r, echo=FALSE}
#   l.seq <- seq(0, 60000, 1000)
#   d <- ncol(costs) 
#   p <- c()
#   p.ce <- matrix(ncol = d, nrow = length(l.seq))
#   for (i in 1:length(l.seq)) {
#     lambda.int <- l.seq[i]
#     inb.int <- as.matrix(effects) * lambda.int - as.matrix(costs)
#     inb.int <- inb.int - inb.int[, 1]
#     inb.int
#     for(j in 1:d) {
#       p.ce[i, j] <- sum(apply(inb.int, 1, which.max) == j) / nrow(inb.int)
#     }
#   }  
#   ceac.int<-list(p=p.ce, l.seq=l.seq, d=d)
#   plot(ceac.int$l.seq, ceac.int$p[, 1], type="l", ylim=c(0,1))
#   for (i in 2:ceac.int$d){
#     lines(ceac.int$l.seq, ceac.int$p[, i], col = i)
#   }
#   abline(v=input$lambdaOverall, lty=2)
#   for (i in 1:ceac.int$d){  
#     points(input$lambdaOverall, ceac.int$p[which(ceac.int$l.seq == input$lambdaOverall), i], pch=20, col="black")
#     text(input$lambdaOverall, ceac.int$p[which(ceac.int$l.seq == input$lambdaOverall), i], ceac.int$p[which(ceac.int$l.seq == input$lambdaOverall), i], pos=1, offset=0.1, cex=0.7)
#   }
#   legend("topright", colnames(costs), col = c(1:i), lty = 1)
ceac.obj <- try(get("ceac.obj"), silent = TRUE)
if (!class(ceac.obj)=="try-error") {
 makeCeacPlot(ceac.obj, input$lambdaOverall, colnames(costs))
}
```

More details for how to interpret CEACs are available from the literature

Fenwick & Byford. (2005) A guide to cost-effectiveness acceptability curves. The British Journal of Psychiatry. 187: 106-108.

## 2.4 Net Benefit of Each Strategy
## 2.4.1 Absolute Net Benefit
Net benefit (NB) is a calculation to put the costs and the `r input$unitBens`s onto the same scale.  This is done by calculating the monetary value of the `r input$unitBens`s using a simple multiplication i.e. `r input$unitBens`s * lambda=`r input$currency` `r format(input$lambdaOverall, digits = 4)` per `r input$unitBens`, where 

```{r, echo=FALSE}
#$$
#NB= `r input$unitBens`*\lambda - Costs
#$$
```

This is particularly useful when comparing several strategies because the analyst and decision maker can see in one single measure the expected net value of each strategy, rather than looking at many comparisons of incremental cost-effectiveness ratios between different options.  Under the rules of decision theory, the strategy with the highest expected net benefit is the one which a decision maker would choose as the optimal startegy.


```{r, echo=FALSE, results='asis'}
makeTableNetBenefit <- function(costs.int, effects.int, lambda, nInt) {
  
  tableNetBenefit <- matrix(NA, ncol= nInt, nrow = 8) 
  for (i in 1:nInt) {
    tableNetBenefit[1,i] <- format(mean(effects.int[,i]), digits=2, nsmall=4)
    tableNetBenefit[2,i] <- format(mean(costs.int[,i]), digits=2, nsmall=2)
    tableNetBenefit[3,i] <- format(mean(effects.int[,i] * lambda - costs.int[,i]), digits=2, nsmall=2)
    tableNetBenefit[4,i] <- format(quantile(effects.int[,i] * lambda - costs.int[,i], 0.025), digits=2, nsmall=2)
    tableNetBenefit[5,i] <- format(quantile(effects.int[,i] * lambda - costs.int[,i], 0.975), digits=2, nsmall=2) 
    tableNetBenefit[6,i] <- format(mean(effects.int[,i] - (costs.int[,i] / lambda)), digits=2, nsmall=4)
    tableNetBenefit[7,i] <- format(quantile(effects.int[,i] - (costs.int[,i] / lambda), 0.025), digits=2, nsmall=4)
    tableNetBenefit[8,i] <- format(quantile(effects.int[,i] - (costs.int[,i] / lambda), 0.975), digits=2, nsmall=4)
  }
  colnames(tableNetBenefit) <- colnames(costs.int)
  rownames(tableNetBenefit) <- c(paste("Mean", input$effectDef), 
                               paste("Mean", input$costDef), 
                               paste("Expected Net Benefit at", 
                                          input$currency, input$lambdaOverall, "per", input$unitBens), 
                               "95% Lower CI (on Costs Scale)", 
                               "95% Upper CI (on Costs Scale)", 
                               "Expected Net Benefit on Effects Scale", 
                               "95% Lower CI (on Effects Scale)", 
                               "95% Upper CI (on Effects Scale)")
  tableNetBenefit
}
print(kable(makeTableNetBenefit(costs, effects, input$lambdaOverall , nInt)))
```

## 2.4.2 Incremental Net Benefit of compared with the optimal comparator
The graph shows the incremental expected net benefit of the strategies compared with `r maxInt`.  

```{r, echo=FALSE}
  lambda<-input$lambdaOverall
  nb <- as.matrix(effects) * lambda - as.matrix(costs)
  c <- which.max(as.matrix(colMeans(nb)))
  inbOpt <- nb-nb[,c]
  means <- colMeans(inbOpt)
  sd <- apply(inbOpt, 2, sd)
  lCI <- apply(inbOpt, 2, quantile, 0.025)
  uCI <- apply(inbOpt, 2, quantile, 0.975)
  colnames(inbOpt) <- colnames(nb)
  mp <- barplot(means, 
          main = paste("Expected Incremental Net Benefit vs. Optimal Strategy\nOptimal Strategy is Strategy",c), 
          xlab = "Strategy", ylab = "INB vs. Optimal Strategy", ylim = c(min(lCI), max(uCI)),
          col=0, border=0, names.arg = 1:length(lCI)) 
  segments(mp - 0.2, means, mp + 0.2, means, lwd=2)
  segments(mp, lCI, mp, uCI, lwd=2)
  segments(mp - 0.1, lCI, mp + 0.1, lCI, lwd=2)
  segments(mp - 0.1, uCI, mp + 0.1, uCI, lwd=2)
  abline(h=0, lty=2)

```

## 2.5 Net Benefit Density Plots
## 2.5.1 Absolute Net Benefit
The absolute monetary net benefit density is calculated for each of the `r nInt` strategy comparators. The absolute Net Benefit density plot illustrates the overlaid densities for the `r nIterate` simulation runs in the Probabistic Sensitivity Analysis. This graph illustrates how much overlap their is in the simulated Net Benefit of all strategies. However, the overlap between densities may be due to correlation in simulated outcomes, therefore it is necessary to examine the incremental differences between strategies (as discussed in Naversnik K, 2014).

```{r, echo=FALSE}
  lambda<-input$lambdaOverall
  nb <- as.matrix(effects) * lambda - as.matrix(costs)
  d <- ncol(costs) 
  xmax<-max(nb) + 0.1 * (max(nb) - min(nb))
  xmin<-min(nb) - 0.1 * (max(nb) - min(nb))
  ymax<-c(1:d)
  for (i in 1:d){
    den<-density(nb[, i])
    ymax[i]<-max(den$y)
  }
  ymax<-max(ymax)
  plot(density(nb[, 1]), type = "l", col = 1, xlim = c(xmin, xmax), ylim = c(0, ymax), 
       xlab="Net Benefit",main="Net Benefit Densities")
  for (i in 2:d){
    lines(density(nb[, i]), col = i, lty = i)
  }
  # Need strategy names adding
  legend("topleft", colnames(nb), col=c(1:d), lty = 1:d, cex=0.7)
```


## 2.5.2 Incremental Net Benefit Density Compared with Optimal Strategy
Densities for the incremental net benefit of each strategy compared with `r maxInt` (the strategy with maximum expected net benefit) are presented. In this graph it is possible compare strategy densities with correlation removed. It is possible to observe which strategies have simulated Net Benefit greater than the optimal strategy. If there are several strategies with overlapping densities, then several strategies are close in terms of their expected value to a decision maker, and given the relatively large decision uncertainty it might be valuable to consider further research to reduce uncertainty.  The value of reducing uncertainty to the decision maker by undertaking further research is the subject of the analyses using expected value of information calculations.  These calculations can consider all decision uncertainty (the overall expected value of perfect information (EVPI)) or for particular uncertain parameters within the PSA (expected value of perfect parameter information (EVPPI)). 

```{r, echo=FALSE}
  lambda<-input$lambdaOverall
  nb <- as.matrix(effects) * lambda - as.matrix(costs)
  c <- which.max(as.matrix(colMeans(nb)))
  inbOpt <- nb - nb[,c]
  inbOpt <- as.matrix(inbOpt[, -c])
  colnames(inbOpt) <- colnames(nb)[-c]
  d <- ncol(inbOpt) + ifelse(FALSE, 1, 0) # what does this ifelse do?
  xmax <- max(inbOpt) + 0.1 * (max(inbOpt) - min(inbOpt))
  xmin <- min(inbOpt) - 0.1 * (max(inbOpt) - min(inbOpt))
  ymax <- 1:d
  for (i in 1:d){
    den <- density(inbOpt[, i])
    ymax[i] <- max(den$y)
  }
  ymax <- max(ymax)
  plot(density(inbOpt[, 1]), type = "l", col = 1, xlim = c(xmin, xmax), 
       ylim = c(0, ymax), xlab="INB vs. Optimal Strategy",
       main="Incremental Net Benefit Density")
  if (d>1) {
    for (i in 2:d){
      lines(density(inbOpt[, i]), col = i, lty = i)
    }    
  }
  # Need strategy names adding
  legend("topleft", colnames(inbOpt), col=1:d, lty = 1:d, cex=0.7)
  abline(v=0, lty=2)
```

More information about illustrating uncertainty for multiple strategies with correlated output are available in the literature.

Naversnik K (2014) Output correlations in probabilistic models with multiple alternatives. Eur J Health Econ. 2014 Jan 4.

# 3. Putting a value on the decision uncertainty: Overall Expected Value of Perfect Information Calculation
## 3.1 Understanding the EVPI
The calculation begins with the existing confidence intervals (or credible intervals) for the model parameters as used in the probabilistic sensitivity analysis.  We then imagine a world in which we become absolutely (perfectly) certain about all of the model parameters i.e. the confidence interval for every single parameter is zero.  The decision maker would then be absolutely certain which strategy to select and would choose the one with highest net benefit.  One can visualise this idea by imagining that instead of seeing the cloud of dots on the cost-effectiveness plane (representing current uncertainty in costs and benefits) and having to choose, the decision maker now knows exactly which dot is the true value (because all of the uncertainty is removed) and so can be certain to choose the strategy which gives the best net benefit. In a two strategy comparison of new versus current care, if the true dot turns out to be below and to the right of the thresholdlambda line, then the decision maker would select the new strategy.  If the true dot is above and to the left, then current care would be selected.  Under the current uncertainty, the decision maker will choose the strategy based on the expected costs and benefits (essentially on whether the centre of gravity of the cloud is above or below the threshold line). 

## 3.2 Overall EVPI


```{r, echo=FALSE}
EvpiBenefitPerson<-EvpiCostPerson<-EvpiCostPop<-prevalence<-EvpiCostPopYears<-99
```
The overall EVPI per person affected by the decision is estimated at `r input$currency` `r EvpiCostPerson` per person.  This is equivalent to `r EvpiBenefitPerson` `r input$unitBens` per person in decision uncertainty when valuing uncertainty on the `r input$unitBens` scale.

Assuming an annual number of people affected by the decision of `r input$annualPrev`, the overall EVPI per year is `r input$currency` `r EvpiCostPop` for `r input$juristiction`. 

When thinking about the overall expected value of removing decision uncertainty, one needs to consider how long the current comparison will remain relevant e.g. if new treatments of options or even cures are anticipated to become available for a disease.  For the specified decision relevance horizon of `r input$horizon` years, the overall expected value of removing decision uncertainty for `r input$juristiction` would in total be `r input$currency` `r EvpiCostPopYears`.

Research or data collection exercises costing more than this amount would not be considered cost-effective use of resources. This is because the return on investment from the research, as measured by the health gain and cost savings of enabling decision makers ability to switch and select other strategies when evidence obtained reduces decision uncertainty, is expected to be no higher than the figure of `r input$currency` `r EvpiCostPopYears`.

The EVPI estimates in the table below quantifies the expected value to decision makers within the jurisdiction of removing all current decision uncertainty at a threshold of `r input$currency` `r format(input$lambdaOverall,digits=4)` per `r input$unitBens`.  This will enable comparison against previous analyses to provide an idea of the scale of decision uncertainty in this topic compared with other previous decisions. The EVPI estimate for varying willingness to pay thresholds are illustrated in the figures below. 

```{r, echo=FALSE, results='asis'}
     tableEVPI <- matrix(NA, nrow = 7, ncol = 2)
     colnames(tableEVPI) <- c(paste("EVPI Financial Valuation (",input$currency,")"), paste("EVPI", input$unitBens, "Valuation"))
     rownames(tableEVPI) <- c("Per Person Affected by the Decision", 
                              paste("Assuming", input$annualPrev, "Persons Affected per Year"), 
                              "Over 5 Years", 
                              "Over 10 Years", 
                              "Over 15 Years", 
                              "Over 20 years", 
                              paste("Over Decision Relevance Horizon (",input$horizon, "years)"))
     
     overallEvpi <- calcEvpi(get("costs", envir=cache), get("effects", envir=cache), 
              lambda=input$lambdaOverall)
     assign("overallEvpi", overallEvpi, envir = cache)
     assign("lambdaOverall", input$lambdaOverall, envir = cache)
     evpiVector <- c(overallEvpi, overallEvpi * input$annualPrev, overallEvpi * input$annualPrev * 5, 
                     overallEvpi * input$annualPrev * 10, overallEvpi * input$annualPrev * 15,
                     overallEvpi * input$annualPrev * 20,
                     overallEvpi * input$annualPrev * input$horizon)     
     tableEVPI[, 1] <- signif(evpiVector, 4)          
     tableEVPI[, 2] <- signif(evpiVector / input$lambdaOverall, 4)   
     print(kable(tableEVPI))
```

```{r, echo=FALSE}
  ## makes the overall EVPI plot
  lambda.int<-input$lambdaOverall 
  l.seq <- seq(0, lambda * 10, lambda / 20)
  p <- c()
  for (lambda.int in l.seq) {
    inb.int <- data.frame(as.matrix(effects) * lambda.int - as.matrix(costs))

    evpi <- mean(do.call(pmax, inb.int)) - max(colMeans(inb.int))
    p <- c(p, evpi)
  }
  plot(l.seq, p, type="l", main = paste("Overall EVPI per person ", input$currency), xlab = "Threshold willingness to pay", ylab = paste("Annual population EVPI ", input$currency))
  abline(v=lambda, lty=2)
  points(lambda, p[which(l.seq == lambda)], pch=20, col="black")
  #text(lambda, p[which(l.seq == lambda)], round(p[which(l.seq == lambda)],2), pos=1, offset=0.1)
  
```

```{r, echo=FALSE}
  ## makes the overall EVPI plot
  lambda.int<-input$lambdaOverall 
  l.seq <- seq(0, lambda * 10, lambda / 20)
  p <- c()
  for (lambda.int in l.seq) {
    inb.int <- as.matrix(effects) * lambda.int - as.matrix(costs)
    inb.int <- inb.int - inb.int[, 1]
    #inb.int
    evpi <- (mean(pmax(inb.int[, 2], inb.int[, 1])) - max(colMeans(inb.int)))*prevalence
    p <- c(p, evpi)
  }  
  plot(l.seq, p, type="l", main = paste("Overall EVPI per annual prevalence ", input$currency), xlab = "Threshold willingness to pay", ylab = paste("Annual population EVPI ", input$currency))
  abline(v=lambda, lty=2)
  points(lambda, p[which(l.seq == lambda)], pch=20, col="black")
  #text(lambda, p[which(l.seq == lambda)], round(p[which(l.seq == lambda)],0), pos=1, offset=0.1)
  
```

```{r, echo=FALSE}
  ## makes the overall EVPI plot
  lambda.int<-input$lambdaOverall 
  l.seq <- seq(0, lambda * 10, lambda / 20)
  p <- c()
  for (lambda.int in l.seq) {
    inb.int <- as.matrix(effects) * lambda.int - as.matrix(costs)
    inb.int <- inb.int - inb.int[, 1]
    #inb.int
    evpi <- (mean(pmax(inb.int[, 2], inb.int[, 1])) - max(colMeans(inb.int))) * input$annualPrev
    evpi <- evpi / lambda.int
    p <- c(p, evpi)
  }  
  plot(l.seq, p, type="l", main = paste("Overall EVPI per annual prevalence ", input$unitBens), xlab = "Threshold willingness to pay", ylab = paste("Annual population EVPI ", input$unitBens))
  abline(v=lambda, lty=2)
  points(lambda, p[which(l.seq == lambda)], pch=20, col="black")
  #text(lambda, p[which(l.seq == lambda)], round(p[which(l.seq == lambda)],0), pos=1, offset=0.1)
```

```{r, echo=FALSE}
  l.seq <- seq(0, lambda * 10, lambda / 20)
  p <- c()
  for (lambda.int in l.seq) {
    inb.int <- as.matrix(effects) * lambda.int - as.matrix(costs)
    inb.int <- inb.int - inb.int[, 1]
    #inb.int
    evpi <- (mean(pmax(inb.int[, 2], inb.int[, 1])) - max(colMeans(inb.int))) * input$annualPrev * input$horizon
    p <- c(p, evpi)
  }  
  plot(l.seq, p, type="l", main = paste("Overall EVPI over decision relevance ", input$currency), xlab = "Threshold willingness to pay", ylab = paste("Annual population EVPI ", input$currency))
  abline(v=lambda, lty=2)
  points(lambda, p[which(l.seq == lambda)], pch=20, col="black")
  #text(lambda, p[which(l.seq == lambda)], round(p[which(l.seq == lambda)],0),pos=1, offset=0.1)
```

# 4. Which parameters are causing most of the decision uncertainty and what is the potential value of reducing uncertainty by collecting more data: Partial Expected Value of Perfect Information
## 4.1 Single parameter EVPPI

```{r, echo=FALSE, results='asis'}
     lambda <- input$lambdaOverall # re-run if labmda changes
     assign("lambdaOverall", input$lambdaOverall, envir = cache)
     params <- get("params", envir=cache)
     costs <- get("costs", envir=cache)
     effects <- get("effects", envir=cache)

     overallEvpi <- calcEvpi(costs, effects, lambda)
     assign("overallEvpi", overallEvpi, envir = cache)
     
     inb <- createInb(costs, effects, lambda)
     pEVPI <- applyCalcSingleParamGam(params, inb)
     assign("pEVPI", pEVPI, envir=cache)
     
     tableEVPPI <- matrix(NA, nrow = ncol(params), ncol = 4)
     tableEVPPI[, 1] <- round(pEVPI, 2)
     tableEVPPI[, 2] <- round(pEVPI / overallEvpi , 2)
     tableEVPPI[, 3] <- signif(pEVPI * input$annualPrev, 4)
     tableEVPPI[, 4] <- signif(pEVPI * input$annualPrev * input$horizon, 4)
     colnames(tableEVPPI) <- c(paste("Per Person EVPPI (", input$currency, ")"), "Indexed Overall EVPI = 1.00", 
                               paste("EVPPI for ", input$jurisdiction, " Per Year"), 
                               paste("EVPPI for ", input$jurisdiction, " over ", input$horizon, " years", sep=""))
     rownames(tableEVPPI) <- colnames(get("params", envir=cache))
     print(kable(tableEVPPI))
```

```{r, echo=FALSE}
  EVPPI <- matrix(pEVPI[order(pEVPI)], ncol = length(pEVPI), nrow = 1)
  colnames(EVPPI)<-colnames(params[order(pEVPI)])
  barplot(EVPPI, horiz = TRUE, cex.names=0.7, las=1, main= "Single parameter Partial EVPI per person", xlab = "Partial EVPI per person")  
  
```

## 4.1 Group parameter EVPPI

```{r, echo=FALSE, results='asis'}
if(!is.null(setStoreMatchEvpiValues)) {
tableEvpiValues <- cbind(setStoreMatchEvpiValues, subsetEvpiValues)
colnames(tableEvpiValues) <- c("Parameters","Partial EVPI per person","standard error")
print(kable(tableEvpiValues))
}
```



